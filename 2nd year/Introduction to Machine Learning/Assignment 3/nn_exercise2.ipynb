{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67d522bf",
   "metadata": {},
   "source": [
    "## Exercise 2, Neural networks 'by hand'\n",
    "The exercise is two-parted. First is to fill in the 'missing' code. The overall problem in the first part is to solve the XOR-problem using a neural network.\n",
    "\n",
    "In the second part you should copy your functional code into a new cell, and decrease the initailized weights by a factor of 1/10. What happens? Do you think that the problem is fixable in some way? How? Document your thoughts in a short text either in the notebook (preferred) or separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f66bafd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target shape: (1, 4)\n",
      "Epoch 0, Loss: 1.009573924852373\n",
      "Epoch 1000, Loss: 0.6815341543641776\n",
      "Epoch 2000, Loss: 0.6922352896946985\n",
      "Epoch 3000, Loss: 0.6860167736930206\n",
      "Epoch 4000, Loss: 0.6567411499644419\n",
      "Epoch 5000, Loss: 0.4200364652869539\n",
      "Epoch 6000, Loss: 0.19016445727006204\n",
      "Epoch 7000, Loss: 0.13204645481397737\n",
      "Epoch 8000, Loss: 0.10306359093282765\n",
      "Epoch 9000, Loss: 0.08446857380374213\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sigmoid activation\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# derivative of sigmoid\n",
    "def sigmoid_derivative(x):\n",
    "    # fill in your code here\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# cross-entropy loss function\n",
    "def cross_netropy_loss(out_nn, target):\n",
    "    # fill in your code here\n",
    "    return -(1/target.shape[1]) * np.sum(target * np.log(out_nn) + (1 - target)*np.log(1 - out_nn))\n",
    "\n",
    "\n",
    "# def cross_netropy_loss(out_nn, target):\n",
    "#     t = np.float_(target)\n",
    "#     p = np.float_(out_nn)\n",
    "    \n",
    "#     return -(1/m)np.sum(t * np.log(p) + (1 - t) * np.log(1 - p))\n",
    "\n",
    "\n",
    "# Initialize weights and biases\n",
    "np.random.seed(1)\n",
    "W1 = np.random.randn(2, 2)\n",
    "b1 = np.random.randn(2, 1)\n",
    "W2 = np.random.randn(1, 2)\n",
    "b2 = np.random.randn(1, 1)\n",
    "\n",
    "# xor data\n",
    "X = np.array([[0,0,1,1],[0,1,0,1]])\n",
    "target = np.array([[0, 1, 1, 0]])\n",
    "\n",
    "# forward\n",
    "def forward(X):\n",
    "    # fill in your code here\n",
    "    z1 = np.dot(W1, X) + b1\n",
    "    Q1 = sigmoid(z1)\n",
    "    z2 = np.dot(W2, Q1) + b2\n",
    "    out = sigmoid(z2)\n",
    "    # the return should be the output of the network and the\n",
    "    # hidden output which are both used in the backprop\n",
    "    return out, Q1\n",
    "\n",
    "# backpropagation\n",
    "def backprop(X, target, out_nn, Q1):\n",
    "    delta2 = out_nn - target\n",
    "    delta1 = (W2.T@delta2) * sigmoid_derivative(Q1)\n",
    "    dW2 = delta2@Q1.T\n",
    "    dW1 = delta1@X.T\n",
    "    db2 = np.mean(delta2, axis=1, keepdims=True)\n",
    "    db1 = np.mean(delta1, axis=1, keepdims=True)\n",
    "    return dW2, dW1, db2, db1\n",
    "\n",
    "# updating the weights\n",
    "def update(W2, W1, b2, b1, dW2, dW1, db2, db1, alpha):\n",
    "    # fill in your code here\n",
    "    W1 = W1 - alpha * dW1\n",
    "    W2 = W2 - alpha * dW2\n",
    "    b1 = b1 - alpha * db1\n",
    "    b2 = b2 - alpha * db2\n",
    "    return W1, W2, b1, b2\n",
    "\n",
    "# learning rate\n",
    "alpha = 0.1\n",
    "\n",
    "# print the prediction of the XOR BEFORE training here\n",
    "\n",
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    # fill in your code here!\n",
    "    out, Q1 = forward(X)\n",
    "    loss = cross_netropy_loss(out, target)\n",
    "    dW2, dW1, db2, db1 = backprop(X, target, out, Q1)\n",
    "    W1, W2, b1, b2 = update(W2, W1, b2, b1, dW2, dW1, db2, db1, alpha)\n",
    "    # use the above functions to evaluate and update the network\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# print the prediction of the XOR AFTER training here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc44a0c1-1241-4160-b154-e159092cbb01",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfb735a8-8bc5-4a2f-9651-da855b5526d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and biases\n",
    "np.random.seed(1)\n",
    "W1 = np.random.randn(2, 2) * 0.1\n",
    "b1 = np.random.randn(2, 1)\n",
    "W2 = np.random.randn(1, 2) * 0.1\n",
    "b2 = np.random.randn(1, 1)\n",
    "\n",
    "# xor data\n",
    "X = np.array([[0,0,1,1],[0,1,0,1]])\n",
    "target = np.array([[0, 1, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1910c2e1-90ac-4cf6-b6c9-44e864dbd5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7168639936612257\n",
      "Epoch 1000, Loss: 0.6931067577356147\n",
      "Epoch 2000, Loss: 0.6930896310884921\n",
      "Epoch 3000, Loss: 0.6930660068605172\n",
      "Epoch 4000, Loss: 0.6930320979098346\n",
      "Epoch 5000, Loss: 0.6929827231426999\n",
      "Epoch 6000, Loss: 0.6929121315287233\n",
      "Epoch 7000, Loss: 0.6928181229733131\n",
      "Epoch 8000, Loss: 0.6927119635231196\n",
      "Epoch 9000, Loss: 0.6926286091190024\n"
     ]
    }
   ],
   "source": [
    "# learning rate\n",
    "alpha = 0.1\n",
    "\n",
    "# print the prediction of the XOR BEFORE training here\n",
    "\n",
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    # fill in your code here!\n",
    "    out, Q1 = forward(X)\n",
    "    loss = cross_netropy_loss(out, target)\n",
    "    dW2, dW1, db2, db1 = backprop(X, target, out, Q1)\n",
    "    W1, W2, b1, b2 = update(W2, W1, b2, b1, dW2, dW1, db2, db1, alpha)\n",
    "    # use the above functions to evaluate and update the network\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72178421-45e0-4006-81aa-b0e914a86b43",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d1bf6a-d392-4e4e-8c23-8275db576ec1",
   "metadata": {},
   "source": [
    "The effect that it has is that it has hard time reducing the loss. After some research, it appears that this problem is called \"The vanishing gradient problem\" and refers to the fact that \"the gradient propagated backwards through the layers become very small, making it difficult for the network to update\". Source : https://medium.com/@amanatulla1606/vanishing-gradient-problem-in-deep-learning-understanding-intuition-and-solutions-da90ef4ecb54#:~:text=A1%3A%20The%20vanishing%20gradient%20problem,to%20update%20the%20weights%20effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33e2589-2df2-43bd-b969-e369302deae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
